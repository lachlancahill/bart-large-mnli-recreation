{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    set_seed,\n",
    ")\n",
    "from tqdm.auto import tqdm\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from config import model_checkpoint\n",
    "\n",
    "import evaluate"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-11T21:01:31.122770Z",
     "start_time": "2024-06-11T21:00:41.358982Z"
    }
   },
   "id": "39f76e8f4d3eafdc",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_datasets=DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
      "        num_rows: 392702\n",
      "    })\n",
      "    validation_matched: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
      "        num_rows: 9815\n",
      "    })\n",
      "    validation_mismatched: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
      "        num_rows: 9832\n",
      "    })\n",
      "    test_matched: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
      "        num_rows: 9796\n",
      "    })\n",
      "    test_mismatched: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
      "        num_rows: 9847\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mnli\")\n",
    "\n",
    "print(f'{raw_datasets=}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-11T21:03:12.303785Z",
     "start_time": "2024-06-11T21:03:02.351358Z"
    }
   },
   "id": "160b01a8661b05e6",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    outputs = tokenizer(\n",
    "        examples[\"premise\"], \n",
    "        examples[\"hypothesis\"], \n",
    "        truncation='only_first', \n",
    "        padding=\"max_length\",\n",
    "        max_length=1024\n",
    "    )  # TODO: make max length dynamic.\n",
    "    return outputs\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True, remove_columns=[\"idx\", \"premise\", \"hypothesis\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b4c61b435ab74f91",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=3)\n",
    "\n",
    "train_batch_size = 8\n",
    "eval_batch_size = 8\n",
    "\n",
    "\n",
    "def create_dataloaders(train_batch_size=train_batch_size, eval_batch_size=eval_batch_size):\n",
    "    train_dataloader = DataLoader(\n",
    "        # TODO: Remove filter on training data.\n",
    "        Subset(tokenized_datasets[\"train\"], range(160_000)), shuffle=False, batch_size=train_batch_size\n",
    "    )\n",
    "    eval_matched_dataloader = DataLoader(\n",
    "        tokenized_datasets[\"validation_matched\"], shuffle=False, batch_size=eval_batch_size\n",
    "    )\n",
    "    eval_mismatched_dataloader = DataLoader(\n",
    "        tokenized_datasets[\"validation_mismatched\"], shuffle=False, batch_size=eval_batch_size\n",
    "    )\n",
    "    return train_dataloader, eval_matched_dataloader, eval_mismatched_dataloader\n",
    "\n",
    "\n",
    "train_dataloader, eval_dataloader, eval_mismatched_dataloader = create_dataloaders()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e4dba334e6438a9e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "metric = evaluate.load(\"glue\", \"mnli\", trust_remote_code=True)\n",
    "\n",
    "# predictions = outputs.logits.detach().argmax(dim=-1)\n",
    "# metric.compute(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "hyperparameters = {\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"num_epochs\": 3,\n",
    "    \"train_batch_size\": train_batch_size,  # Actual batch size will this x 8\n",
    "    \"eval_batch_size\": eval_batch_size,  # Actual batch size will this x 8\n",
    "    'gradient_accumulation_steps': 2,\n",
    "    \"seed\": 42,\n",
    "}\n",
    "\n",
    "\n",
    "def training_function(model):\n",
    "    # Initialize accelerator\n",
    "    accelerator = Accelerator()\n",
    "\n",
    "    # To have only one message (and not 8) per logs of Transformers or Datasets, we set the logging verbosity\n",
    "    # to INFO for the main process only.\n",
    "    # if accelerator.is_main_process:\n",
    "    #     datasets.utils.logging.set_verbosity_warning()\n",
    "    #     transformers.utils.logging.set_verbosity_info()\n",
    "    # else:\n",
    "    #     datasets.utils.logging.set_verbosity_error()\n",
    "    #     transformers.utils.logging.set_verbosity_error()\n",
    "\n",
    "    train_dataloader, eval_dataloader, eval_mismatched_dataloader = create_dataloaders(\n",
    "        train_batch_size=hyperparameters[\"train_batch_size\"], eval_batch_size=hyperparameters[\"eval_batch_size\"]\n",
    "    )\n",
    "    # The seed need to be set before we instantiate the model, as it will determine the random head.\n",
    "    set_seed(hyperparameters[\"seed\"])\n",
    "\n",
    "    # Instantiate optimizer\n",
    "    optimizer = AdamW(params=model.parameters(), lr=hyperparameters[\"learning_rate\"])\n",
    "\n",
    "    # Prepare everything\n",
    "    # There is no specific order to remember, we just need to unpack the objects in the same order we gave them to the\n",
    "    # prepare method.\n",
    "    model, optimizer, train_dataloader, eval_dataloader, eval_mismatched_dataloader = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, eval_dataloader, eval_mismatched_dataloader\n",
    "    )\n",
    "\n",
    "    num_epochs = hyperparameters[\"num_epochs\"]\n",
    "    gradient_accumulation_steps = hyperparameters[\"gradient_accumulation_steps\"]\n",
    "    # Instantiate learning rate scheduler after preparing the training dataloader as the prepare method\n",
    "    # may change its length.\n",
    "    lr_scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=100,\n",
    "        num_training_steps=(len(train_dataloader) // gradient_accumulation_steps) * num_epochs,\n",
    "    )\n",
    "\n",
    "    # Instantiate a progress bar to keep track of training. Note that we only enable it on the main\n",
    "    # process to avoid having 8 progress bars.\n",
    "    progress_bar = tqdm(range(num_epochs * len(train_dataloader)), disable=not accelerator.is_main_process)\n",
    "    # Now we train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "            accelerator.backward(loss)\n",
    "\n",
    "            if (step + 1) % gradient_accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                progress_bar.update(gradient_accumulation_steps)\n",
    "\n",
    "        model.eval()\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "\n",
    "        for step, batch in enumerate(eval_dataloader):\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "            predictions = outputs.logits.argmax(dim=-1)\n",
    "\n",
    "            # We gather predictions and labels from the 8 TPUs to have them all.\n",
    "            all_predictions.append(accelerator.gather(predictions))\n",
    "            all_labels.append(accelerator.gather(batch[\"labels\"]))\n",
    "\n",
    "        # Concatenate all predictions and labels.\n",
    "        # The last thing we need to do is to truncate the predictions and labels we concatenated\n",
    "        # together as the prepared evaluation dataloader has a little bit more elements to make\n",
    "        # batches of the same size on each process.\n",
    "        all_predictions = torch.cat(all_predictions)[:len(tokenized_datasets[\"validation_matched\"])]\n",
    "        all_labels = torch.cat(all_labels)[:len(tokenized_datasets[\"validation_matched\"])]\n",
    "\n",
    "        eval_metric = metric.compute(predictions=all_predictions, references=all_labels)\n",
    "\n",
    "        # Use accelerator.print to print only on the main process.\n",
    "        accelerator.print(f\"epoch {epoch}:\", eval_metric)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe08dc36f5af9e43",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "# training_function(model)\n",
    "\n",
    "from accelerate import notebook_launcher\n",
    "\n",
    "notebook_launcher(training_function, (model,), num_processes=2)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c2a2a6799c882b0",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
