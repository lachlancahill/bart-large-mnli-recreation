{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-15T12:36:46.040893Z",
     "start_time": "2024-06-15T12:36:42.649302Z"
    }
   },
   "source": "from transformers import AutoTokenizer, BartTokenizerFast, CodeGenTokenizerFast, PreTrainedTokenizerFast, LlamaTokenizerFast, T5TokenizerFast",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T12:37:53.719617Z",
     "start_time": "2024-06-15T12:37:48.103692Z"
    }
   },
   "cell_type": "code",
   "source": [
    "new_repo = 'FacebookAI/roberta-large'\n",
    "\n",
    "bart_large_mnli_tok:BartTokenizerFast = AutoTokenizer.from_pretrained('facebook/bart-large-mnli')\n",
    "bart_large_tok:BartTokenizerFast = AutoTokenizer.from_pretrained('facebook/bart-large')\n",
    "new_tok:T5TokenizerFast = AutoTokenizer.from_pretrained(new_repo)\n",
    "\n",
    "# new_tok.add_eos_token = True\n",
    "# new_tok.add_bos_token = True\n",
    "\n",
    "# new_tok.pad_token = '<PAD>'\n",
    "\n"
   ],
   "id": "d2fd87bab5c0d42b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4dd5d3e5c49648e09d39708f75387a8d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lachl\\PycharmProjects\\bart-large-mnli-recreation\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "eb54f651681a40f88ef7c1873159ec0e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a1ec8a64947c43ff8a99ba6bf16808bd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "24480494659d4d35970349edf8305a08"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cb4139f1eec241b38f77c07e1c11eff6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T12:37:53.735485Z",
     "start_time": "2024-06-15T12:37:53.719617Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"{type(bart_large_mnli_tok)=}\")\n",
    "print(f\"{type(bart_large_tok)=}\")\n",
    "print(f\"{type(new_tok)=}\")"
   ],
   "id": "63497375f023bef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(bart_large_mnli_tok)=<class 'transformers.models.bart.tokenization_bart_fast.BartTokenizerFast'>\n",
      "type(bart_large_tok)=<class 'transformers.models.bart.tokenization_bart_fast.BartTokenizerFast'>\n",
      "type(new_tok)=<class 'transformers.models.roberta.tokenization_roberta_fast.RobertaTokenizerFast'>\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T12:37:57.126316Z",
     "start_time": "2024-06-15T12:37:57.110685Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Pad tokens used\n",
    "print(f\"{bart_large_mnli_tok.pad_token}\")\n",
    "print(f\"{bart_large_tok.pad_token}\")\n",
    "print(f\"{new_tok.pad_token}\")# Pad tokens used# Pad tokens used"
   ],
   "id": "42b205ebc060686a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad>\n",
      "<pad>\n",
      "<pad>\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T12:37:57.520547Z",
     "start_time": "2024-06-15T12:37:57.505014Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Sep tokens used\n",
    "print(f\"{bart_large_mnli_tok.sep_token}\")\n",
    "print(f\"{bart_large_tok.sep_token}\")\n",
    "print(f\"{new_tok.sep_token}\")# Pad tokens used"
   ],
   "id": "cab22c1b1f726d7f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s>\n",
      "</s>\n",
      "</s>\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T12:37:58.172513Z",
     "start_time": "2024-06-15T12:37:58.159493Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"{bart_large_mnli_tok.model_max_length}\")\n",
    "print(f\"{bart_large_tok.model_max_length}\")\n",
    "print(f\"{new_tok.model_max_length}\")"
   ],
   "id": "b52dd931fc60c8a2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n",
      "1024\n",
      "512\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T11:32:29.073653Z",
     "start_time": "2024-06-15T11:32:29.057510Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "premises = [\n",
    "    \"and\",\n",
    "]\n",
    "\n",
    "hypotheses = [\n",
    "    'my'\n",
    "]\n",
    "\n",
    "def apply_tokenizer(tokenizer:PreTrainedTokenizerFast, premises, hypotheses, also_decode=True):\n",
    "    \n",
    "    outputs = tokenizer(premises, hypotheses, truncation='only_first', padding=\"max_length\",\n",
    "                        max_length=25, add_special_tokens=True) \n",
    "    \n",
    "    if also_decode:\n",
    "        outputs['decoded_input_ids'] = [[tokenizer.decode(i) for i in seq] for seq in outputs['input_ids']]\n",
    "    \n",
    "    return outputs\n"
   ],
   "id": "d162110726b24fb1",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T11:32:29.089664Z",
     "start_time": "2024-06-15T11:32:29.073653Z"
    }
   },
   "cell_type": "code",
   "source": "apply_tokenizer(bart_large_mnli_tok, premises, hypotheses)",
   "id": "1c2679fea032beac",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 463, 2, 2, 4783, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'decoded_input_ids': [['<s>', 'and', '</s>', '</s>', 'my', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T11:32:29.105731Z",
     "start_time": "2024-06-15T11:32:29.089664Z"
    }
   },
   "cell_type": "code",
   "source": "apply_tokenizer(bart_large_tok, premises, hypotheses)",
   "id": "692c54b5bc07fa1b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 463, 2, 2, 4783, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'decoded_input_ids': [['<s>', 'and', '</s>', '</s>', 'my', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T11:32:29.121749Z",
     "start_time": "2024-06-15T11:32:29.105731Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# this currently throws an error because we have no padding token, so let's add it!\n",
    "# apply_tokenizer(new_tok, premises, hypotheses)"
   ],
   "id": "3791cb776ff3091e",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T11:32:31.033851Z",
     "start_time": "2024-06-15T11:32:29.124254Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# add pad token to new tokenizer and model\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "new_model = AutoModelForSequenceClassification.from_pretrained(new_repo, num_labels=3)\n",
    "\n",
    "def add_special_tokens_when_missing(tokenizer, model):\n",
    "    \n",
    "    \n",
    "    needs_pad_token = tokenizer.pad_token is None\n",
    "    if needs_pad_token:\n",
    "        num_added_toks = tokenizer.add_special_tokens({'pad_token': '<|pad_token|>'})\n",
    "        assert num_added_toks == 1\n",
    "\n",
    "    needs_sep_token = tokenizer.sep_token is None\n",
    "    if needs_sep_token:\n",
    "        num_added_toks = tokenizer.add_special_tokens({'sep_token': '<|sep_token|>'})\n",
    "        assert num_added_toks == 1\n",
    "\n",
    "    needs_eos_token = tokenizer.eos_token is None\n",
    "    if needs_eos_token:\n",
    "        num_added_toks = tokenizer.add_special_tokens({'eos_token': '<|eos_token|>'})\n",
    "        assert num_added_toks == 1\n",
    "\n",
    "    needs_bos_token = tokenizer.bos_token is None\n",
    "    if needs_eos_token:\n",
    "        num_added_toks = tokenizer.add_special_tokens({'bos_token': '<|bos_token|>'})\n",
    "        assert num_added_toks == 1\n",
    "\n",
    "    if any([needs_pad_token, needs_sep_token, needs_eos_token, needs_bos_token]):\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "        \n",
    "    return tokenizer, model\n",
    "        \n",
    "    \n",
    "new_tok, new_model = add_special_tokens_when_missing(new_tok, new_model)\n",
    "\n",
    "# print(f\"{type(new_model)=}\")"
   ],
   "id": "a45eb832a08e25d9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5ForSequenceClassification were not initialized from the model checkpoint at google-t5/t5-large and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T11:32:31.049511Z",
     "start_time": "2024-06-15T11:32:31.033851Z"
    }
   },
   "cell_type": "code",
   "source": "apply_tokenizer(new_tok, premises, hypotheses)",
   "id": "2868447b1a4b124",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[11, 1, 82, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'decoded_input_ids': [['and', '</s>', 'my', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T11:32:31.641215Z",
     "start_time": "2024-06-15T11:32:31.049511Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "parent_class = type(new_tok)\n",
    "\n",
    "class UpdatedTokenizerFast(T5TokenizerFast):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n",
    "\n",
    "        if token_ids_1 is None: \n",
    "            raise NotImplementedError('This method is designed for zero shot classification, so requires a premise and hypothesis to be passed. No token_ids_1 was passed.')\n",
    "\n",
    "        output = [self.bos_token_id] + token_ids_0 + [self.sep_token_id] + token_ids_1 + [self.eos_token_id]\n",
    "\n",
    "        return output\n",
    "    \n",
    "updated_new_tok = UpdatedTokenizerFast.from_pretrained(new_repo)\n",
    "\n",
    "# updated_new_tok, new_model = add_special_tokens_when_missing(updated_new_tok, new_model)\n",
    "\n",
    "print(f\"{type(updated_new_tok)=}\")"
   ],
   "id": "aec91db376bb0eb9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'T5TokenizerFast'. \n",
      "The class this function is called from is 'UpdatedTokenizerFast'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(updated_new_tok)=<class '__main__.UpdatedTokenizerFast'>\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T11:32:31.657285Z",
     "start_time": "2024-06-15T11:32:31.641215Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# this currently throws an error because we have no padding token, so let's add it!\n",
    "apply_tokenizer(updated_new_tok, premises, hypotheses)"
   ],
   "id": "e0be03b2112f2470",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[11, 1, 82, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'decoded_input_ids': [['and', '</s>', 'my', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T11:33:32.761017Z",
     "start_time": "2024-06-15T11:33:32.751506Z"
    }
   },
   "cell_type": "code",
   "source": "updated_new_tok(premises, hypotheses, mex_length=None)",
   "id": "cbd2b536efbbdd95",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[11, 1, 82, 1]], 'attention_mask': [[1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T11:32:31.688491Z",
     "start_time": "2024-06-15T11:32:31.673295Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "outputs = new_tok(\n",
    "    premises, \n",
    "    hypotheses, \n",
    "    truncation='only_first', \n",
    "    max_length=25,\n",
    "    padding=\"max_length\", \n",
    "    add_special_tokens=True\n",
    ") \n",
    "\n",
    "print(outputs)"
   ],
   "id": "74ee4b8a166a31d2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[11, 1, 82, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T11:32:31.837510Z",
     "start_time": "2024-06-15T11:32:31.688491Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "decoded_input_ids = [new_tok.decode(i) for i in outputs]\n",
    "\n",
    "print(decoded_input_ids)"
   ],
   "id": "720b2744c45681e",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument 'ids': Can't extract `str` to `Vec`",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[34], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m decoded_input_ids \u001B[38;5;241m=\u001B[39m [new_tok\u001B[38;5;241m.\u001B[39mdecode(i) \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m outputs]\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(decoded_input_ids)\n",
      "Cell \u001B[1;32mIn[34], line 1\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[1;32m----> 1\u001B[0m decoded_input_ids \u001B[38;5;241m=\u001B[39m [\u001B[43mnew_tok\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mi\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m outputs]\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(decoded_input_ids)\n",
      "File \u001B[1;32m~\\PycharmProjects\\bart-large-mnli-recreation\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3836\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.decode\u001B[1;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001B[0m\n\u001B[0;32m   3833\u001B[0m \u001B[38;5;66;03m# Convert inputs to python lists\u001B[39;00m\n\u001B[0;32m   3834\u001B[0m token_ids \u001B[38;5;241m=\u001B[39m to_py_obj(token_ids)\n\u001B[1;32m-> 3836\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_decode(\n\u001B[0;32m   3837\u001B[0m     token_ids\u001B[38;5;241m=\u001B[39mtoken_ids,\n\u001B[0;32m   3838\u001B[0m     skip_special_tokens\u001B[38;5;241m=\u001B[39mskip_special_tokens,\n\u001B[0;32m   3839\u001B[0m     clean_up_tokenization_spaces\u001B[38;5;241m=\u001B[39mclean_up_tokenization_spaces,\n\u001B[0;32m   3840\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   3841\u001B[0m )\n",
      "File \u001B[1;32m~\\PycharmProjects\\bart-large-mnli-recreation\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_fast.py:632\u001B[0m, in \u001B[0;36mPreTrainedTokenizerFast._decode\u001B[1;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001B[0m\n\u001B[0;32m    630\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(token_ids, \u001B[38;5;28mint\u001B[39m):\n\u001B[0;32m    631\u001B[0m     token_ids \u001B[38;5;241m=\u001B[39m [token_ids]\n\u001B[1;32m--> 632\u001B[0m text \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_tokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtoken_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mskip_special_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mskip_special_tokens\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    634\u001B[0m clean_up_tokenization_spaces \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    635\u001B[0m     clean_up_tokenization_spaces\n\u001B[0;32m    636\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m clean_up_tokenization_spaces \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    637\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclean_up_tokenization_spaces\n\u001B[0;32m    638\u001B[0m )\n\u001B[0;32m    639\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m clean_up_tokenization_spaces:\n",
      "\u001B[1;31mTypeError\u001B[0m: argument 'ids': Can't extract `str` to `Vec`"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "4bf4eb51036cf6b7",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
