{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-14T10:25:15.300364Z",
     "start_time": "2024-06-14T10:24:57.810388Z"
    }
   },
   "source": "from transformers import AutoTokenizer, BartTokenizerFast, CodeGenTokenizerFast, PreTrainedTokenizerFast, LlamaTokenizerFast",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T10:25:17.256143Z",
     "start_time": "2024-06-14T10:25:15.302790Z"
    }
   },
   "cell_type": "code",
   "source": [
    "new_repo = 'google-t5/t5-large'\n",
    "\n",
    "bart_large_mnli_tok:BartTokenizerFast = AutoTokenizer.from_pretrained('facebook/bart-large-mnli')\n",
    "bart_large_tok:BartTokenizerFast = AutoTokenizer.from_pretrained('facebook/bart-large')\n",
    "new_tok = AutoTokenizer.from_pretrained(new_repo)\n",
    "\n",
    "# new_tok.add_eos_token = True\n",
    "# new_tok.add_bos_token = True\n",
    "\n",
    "# new_tok.pad_token = '<PAD>'\n",
    "\n"
   ],
   "id": "d2fd87bab5c0d42b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/lachl/PycharmProjects/bart-large-mnli-recreation/.lvenv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T10:25:17.261382Z",
     "start_time": "2024-06-14T10:25:17.257615Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"{type(bart_large_mnli_tok)=}\")\n",
    "print(f\"{type(bart_large_tok)=}\")\n",
    "print(f\"{type(new_tok)=}\")"
   ],
   "id": "63497375f023bef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(bart_large_mnli_tok)=<class 'transformers.models.bart.tokenization_bart_fast.BartTokenizerFast'>\n",
      "type(bart_large_tok)=<class 'transformers.models.bart.tokenization_bart_fast.BartTokenizerFast'>\n",
      "type(new_tok)=<class 'transformers.models.t5.tokenization_t5_fast.T5TokenizerFast'>\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T10:25:17.273531Z",
     "start_time": "2024-06-14T10:25:17.263483Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Pad tokens used\n",
    "print(f\"{bart_large_mnli_tok.pad_token}\")\n",
    "print(f\"{bart_large_tok.pad_token}\")\n",
    "print(f\"{new_tok.pad_token}\")# Pad tokens used# Pad tokens used"
   ],
   "id": "42b205ebc060686a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad>\n",
      "<pad>\n",
      "<pad>\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T10:25:17.289Z",
     "start_time": "2024-06-14T10:25:17.274968Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Sep tokens used\n",
    "print(f\"{bart_large_mnli_tok.sep_token}\")\n",
    "print(f\"{bart_large_tok.sep_token}\")\n",
    "print(f\"{new_tok.sep_token}\")# Pad tokens used"
   ],
   "id": "cab22c1b1f726d7f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s>\n",
      "</s>\n",
      "None\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T10:25:17.301031Z",
     "start_time": "2024-06-14T10:25:17.290670Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"{bart_large_mnli_tok.model_max_length}\")\n",
    "print(f\"{bart_large_tok.model_max_length}\")\n",
    "print(f\"{new_tok.model_max_length}\")"
   ],
   "id": "b52dd931fc60c8a2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n",
      "1024\n",
      "1000000000000000019884624838656\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T10:25:17.311951Z",
     "start_time": "2024-06-14T10:25:17.303709Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "premises = [\n",
    "    \"and\",\n",
    "]\n",
    "\n",
    "hypotheses = [\n",
    "    'my'\n",
    "]\n",
    "\n",
    "def apply_tokenizer(tokenizer:PreTrainedTokenizerFast, premises, hypotheses, also_decode=True):\n",
    "    \n",
    "    outputs = tokenizer(premises, hypotheses, truncation='only_first', padding=\"max_length\",\n",
    "                        max_length=25, add_special_tokens=True) \n",
    "    \n",
    "    if also_decode:\n",
    "        outputs['decoded_input_ids'] = [[tokenizer.decode(i) for i in seq] for seq in outputs['input_ids']]\n",
    "    \n",
    "    return outputs\n"
   ],
   "id": "d162110726b24fb1",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T10:25:17.328742Z",
     "start_time": "2024-06-14T10:25:17.313309Z"
    }
   },
   "cell_type": "code",
   "source": "apply_tokenizer(bart_large_mnli_tok, premises, hypotheses)",
   "id": "1c2679fea032beac",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 463, 2, 2, 4783, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'decoded_input_ids': [['<s>', 'and', '</s>', '</s>', 'my', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T10:25:17.338688Z",
     "start_time": "2024-06-14T10:25:17.331583Z"
    }
   },
   "cell_type": "code",
   "source": "apply_tokenizer(bart_large_tok, premises, hypotheses)",
   "id": "692c54b5bc07fa1b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 463, 2, 2, 4783, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'decoded_input_ids': [['<s>', 'and', '</s>', '</s>', 'my', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T10:25:17.348401Z",
     "start_time": "2024-06-14T10:25:17.345682Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# this currently throws an error because we have no padding token, so let's add it!\n",
    "# apply_tokenizer(new_tok, premises, hypotheses)"
   ],
   "id": "3791cb776ff3091e",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T10:26:57.906228Z",
     "start_time": "2024-06-14T10:25:17.349540Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# add pad token to new tokenizer and model\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "new_model = AutoModelForSequenceClassification.from_pretrained(new_repo, num_labels=3)\n",
    "\n",
    "def add_special_tokens_when_missing(tokenizer, model):\n",
    "    \n",
    "    \n",
    "    needs_pad_token = tokenizer.pad_token is None\n",
    "    if needs_pad_token:\n",
    "        num_added_toks = tokenizer.add_special_tokens({'pad_token': '<|pad_token|>'})\n",
    "        assert num_added_toks == 1\n",
    "\n",
    "    needs_sep_token = tokenizer.sep_token is None\n",
    "    if needs_sep_token:\n",
    "        num_added_toks = tokenizer.add_special_tokens({'sep_token': '<|sep_token|>'})\n",
    "        assert num_added_toks == 1\n",
    "\n",
    "    needs_eos_token = tokenizer.eos_token is None\n",
    "    if needs_eos_token:\n",
    "        num_added_toks = tokenizer.add_special_tokens({'eos_token': '<|eos_token|>'})\n",
    "        assert num_added_toks == 1\n",
    "\n",
    "    needs_bos_token = tokenizer.bos_token is None\n",
    "    if needs_eos_token:\n",
    "        num_added_toks = tokenizer.add_special_tokens({'bos_token': '<|bos_token|>'})\n",
    "        assert num_added_toks == 1\n",
    "\n",
    "    if any([needs_pad_token, needs_sep_token, needs_eos_token, needs_bos_token]):\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "        \n",
    "    return tokenizer, model\n",
    "        \n",
    "    \n",
    "new_tok, new_model = add_special_tokens_when_missing(new_tok, new_model)\n",
    "\n",
    "# print(f\"{type(new_model)=}\")"
   ],
   "id": "a45eb832a08e25d9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model.safetensors:  30%|###       | 891M/2.95G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d9346e8c983c4a39859f0c05dd115509"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5ForSequenceClassification were not initialized from the model checkpoint at google-t5/t5-large and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T10:27:37.358126Z",
     "start_time": "2024-06-14T10:27:37.351836Z"
    }
   },
   "cell_type": "code",
   "source": "apply_tokenizer(new_tok, premises, hypotheses)",
   "id": "2868447b1a4b124",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[11, 1, 82, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'decoded_input_ids': [['and', '</s>', 'my', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T10:26:58.514006Z",
     "start_time": "2024-06-14T10:26:57.931407Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "parent_class = type(new_tok)\n",
    "\n",
    "class UpdatedTokenizerFast(parent_class):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n",
    "\n",
    "        if token_ids_1 is None: \n",
    "            raise NotImplementedError('This method is designed for zero shot classification, so requires a premise and hypothesis to be passed. No token_ids_1 was passed.')\n",
    "\n",
    "        output = [self.bos_token_id] + token_ids_0 + [self.sep_token_id] + token_ids_1 + [self.eos_token_id]\n",
    "\n",
    "        return output\n",
    "    \n",
    "updated_new_tok = UpdatedTokenizerFast.from_pretrained(new_repo)\n",
    "\n",
    "updated_new_tok, new_model = add_special_tokens_when_missing(updated_new_tok, new_model)"
   ],
   "id": "aec91db376bb0eb9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'T5TokenizerFast'. \n",
      "The class this function is called from is 'UpdatedTokenizerFast'.\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T10:26:58.518864Z",
     "start_time": "2024-06-14T10:26:58.515546Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# this currently throws an error because we have no padding token, so let's add it!\n",
    "# apply_tokenizer(updated_new_tok, premises, hypotheses)"
   ],
   "id": "e0be03b2112f2470",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T10:26:58.522930Z",
     "start_time": "2024-06-14T10:26:58.520403Z"
    }
   },
   "cell_type": "code",
   "source": "\n",
   "id": "cbd2b536efbbdd95",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T10:26:58.530683Z",
     "start_time": "2024-06-14T10:26:58.524926Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "outputs = new_tok(\n",
    "    premises, \n",
    "    hypotheses, \n",
    "    truncation='only_first', \n",
    "    max_length=25,\n",
    "    padding=\"max_length\", \n",
    "    add_special_tokens=True\n",
    ") \n",
    "\n",
    "print(outputs)"
   ],
   "id": "74ee4b8a166a31d2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[11, 1, 82, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T10:26:59.424392Z",
     "start_time": "2024-06-14T10:26:58.532026Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "decoded_input_ids = [new_tok.decode(i) for i in outputs]\n",
    "\n",
    "print(decoded_input_ids)"
   ],
   "id": "720b2744c45681e",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument 'ids': Can't extract `str` to `Vec`",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[16], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m decoded_input_ids \u001B[38;5;241m=\u001B[39m [new_tok\u001B[38;5;241m.\u001B[39mdecode(i) \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m outputs]\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(decoded_input_ids)\n",
      "Cell \u001B[0;32mIn[16], line 1\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[0;32m----> 1\u001B[0m decoded_input_ids \u001B[38;5;241m=\u001B[39m [\u001B[43mnew_tok\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mi\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m outputs]\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(decoded_input_ids)\n",
      "File \u001B[0;32m/mnt/c/Users/lachl/PycharmProjects/bart-large-mnli-recreation/.lvenv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3836\u001B[0m, in \u001B[0;36mPreTrainedTokenizerBase.decode\u001B[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001B[0m\n\u001B[1;32m   3833\u001B[0m \u001B[38;5;66;03m# Convert inputs to python lists\u001B[39;00m\n\u001B[1;32m   3834\u001B[0m token_ids \u001B[38;5;241m=\u001B[39m to_py_obj(token_ids)\n\u001B[0;32m-> 3836\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_decode\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   3837\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3838\u001B[0m \u001B[43m    \u001B[49m\u001B[43mskip_special_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mskip_special_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3839\u001B[0m \u001B[43m    \u001B[49m\u001B[43mclean_up_tokenization_spaces\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclean_up_tokenization_spaces\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3840\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3841\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/mnt/c/Users/lachl/PycharmProjects/bart-large-mnli-recreation/.lvenv/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:632\u001B[0m, in \u001B[0;36mPreTrainedTokenizerFast._decode\u001B[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001B[0m\n\u001B[1;32m    630\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(token_ids, \u001B[38;5;28mint\u001B[39m):\n\u001B[1;32m    631\u001B[0m     token_ids \u001B[38;5;241m=\u001B[39m [token_ids]\n\u001B[0;32m--> 632\u001B[0m text \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_tokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtoken_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mskip_special_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mskip_special_tokens\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    634\u001B[0m clean_up_tokenization_spaces \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    635\u001B[0m     clean_up_tokenization_spaces\n\u001B[1;32m    636\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m clean_up_tokenization_spaces \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    637\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclean_up_tokenization_spaces\n\u001B[1;32m    638\u001B[0m )\n\u001B[1;32m    639\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m clean_up_tokenization_spaces:\n",
      "\u001B[0;31mTypeError\u001B[0m: argument 'ids': Can't extract `str` to `Vec`"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "4bf4eb51036cf6b7",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
