{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-13T23:16:12.737724Z",
     "start_time": "2024-06-13T23:16:09.540392Z"
    }
   },
   "source": "from transformers import AutoTokenizer, BartTokenizerFast, CodeGenTokenizerFast, PreTrainedTokenizerFast",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T23:16:13.901933Z",
     "start_time": "2024-06-13T23:16:12.738228Z"
    }
   },
   "cell_type": "code",
   "source": [
    "new_repo = 'h2oai/h2o-danube2-1.8b-base'\n",
    "\n",
    "bart_large_mnli_tok:BartTokenizerFast = AutoTokenizer.from_pretrained('facebook/bart-large-mnli')\n",
    "bart_large_tok:BartTokenizerFast = AutoTokenizer.from_pretrained('facebook/bart-large')\n",
    "new_tok:CodeGenTokenizerFast = AutoTokenizer.from_pretrained(new_repo)"
   ],
   "id": "d2fd87bab5c0d42b",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T23:16:13.918015Z",
     "start_time": "2024-06-13T23:16:13.901933Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"{type(bart_large_mnli_tok)=}\")\n",
    "print(f\"{type(bart_large_tok)=}\")\n",
    "print(f\"{type(new_tok)=}\")"
   ],
   "id": "63497375f023bef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(bart_large_mnli_tok)=<class 'transformers.models.bart.tokenization_bart_fast.BartTokenizerFast'>\n",
      "type(bart_large_tok)=<class 'transformers.models.bart.tokenization_bart_fast.BartTokenizerFast'>\n",
      "type(phi_tok)=<class 'transformers.models.codegen.tokenization_codegen_fast.CodeGenTokenizerFast'>\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T23:16:13.934055Z",
     "start_time": "2024-06-13T23:16:13.918015Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Pad tokens used\n",
    "print(f\"{bart_large_mnli_tok.pad_token}\")\n",
    "print(f\"{bart_large_tok.pad_token}\")\n",
    "print(f\"{new_tok.pad_token}\")# Pad tokens used"
   ],
   "id": "cab22c1b1f726d7f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad>\n",
      "<pad>\n",
      "None\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T23:16:13.981853Z",
     "start_time": "2024-06-13T23:16:13.934055Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"{bart_large_mnli_tok.model_max_length}\")\n",
    "print(f\"{bart_large_tok.model_max_length}\")\n",
    "print(f\"{new_tok.model_max_length}\")"
   ],
   "id": "b52dd931fc60c8a2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n",
      "1024\n",
      "2048\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T23:32:04.307093Z",
     "start_time": "2024-06-13T23:32:04.297020Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "premises = [\n",
    "    \"hello\",\n",
    "]\n",
    "\n",
    "hypotheses = [\n",
    "    'my'\n",
    "]\n",
    "\n",
    "def apply_tokenizer(tokenizer:PreTrainedTokenizerFast, premises, hypotheses, also_decode=True):\n",
    "    \n",
    "    outputs = tokenizer(premises, hypotheses, truncation='only_first', padding=\"longest\",\n",
    "                        max_length=tokenizer.model_max_length, verbose=True) \n",
    "    \n",
    "    if also_decode:\n",
    "        outputs['decoded_input_ids'] = [[tokenizer.decode(i) for i in seq] for seq in outputs['input_ids']]\n",
    "    \n",
    "    return outputs\n"
   ],
   "id": "d162110726b24fb1",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T23:32:05.928949Z",
     "start_time": "2024-06-13T23:32:05.917800Z"
    }
   },
   "cell_type": "code",
   "source": "apply_tokenizer(bart_large_mnli_tok, premises, hypotheses)",
   "id": "1c2679fea032beac",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 42891, 2, 2, 4783, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1]], 'decoded_input_ids': [['<s>', 'hello', '</s>', '</s>', 'my', '</s>']]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T23:32:06.378935Z",
     "start_time": "2024-06-13T23:32:06.358060Z"
    }
   },
   "cell_type": "code",
   "source": "apply_tokenizer(bart_large_tok, premises, hypotheses)",
   "id": "692c54b5bc07fa1b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 42891, 2, 2, 4783, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1]], 'decoded_input_ids': [['<s>', 'hello', '</s>', '</s>', 'my', '</s>']]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-13T23:32:07.182870Z",
     "start_time": "2024-06-13T23:32:07.169364Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# this currently throws an error because we have no padding token, so let's add it!\n",
    "apply_tokenizer(new_tok, premises, hypotheses)"
   ],
   "id": "3791cb776ff3091e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[31373, 1820]], 'attention_mask': [[1, 1]], 'decoded_input_ids': [['hello', 'my']]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T00:31:47.308823Z",
     "start_time": "2024-06-14T00:31:45.573683Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# add pad token to phi tokenizer and model\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "phi_model = AutoModelForSequenceClassification.from_pretrained(new_repo, num_labels=3)\n",
    "\n",
    "def add_special_tokens_when_missing(tokenizer, model):\n",
    "    \n",
    "    needs_pad_token = tokenizer.pad_token is None\n",
    "    if needs_pad_token:\n",
    "        num_added_toks = tokenizer.add_special_tokens({'pad_token': '<|pad_token|>'})\n",
    "        assert num_added_toks == 1\n",
    "    \n",
    "    needs_sep_token = tokenizer.sep_token is None\n",
    "    if needs_sep_token:\n",
    "        num_added_toks = tokenizer.add_special_tokens({'sep_token': '<|sep_token|>'})\n",
    "        assert num_added_toks == 1\n",
    "        \n",
    "    needs_eos_token = tokenizer.eos_token is None\n",
    "    if needs_eos_token:\n",
    "        num_added_toks = tokenizer.add_special_tokens({'eos_token': '<|eos_token|>'})\n",
    "        assert num_added_toks == 1\n",
    "        \n",
    "    needs_bos_token = tokenizer.bos_token is None\n",
    "    if needs_eos_token:\n",
    "        num_added_toks = tokenizer.add_special_tokens({'bos_token': '<|bos_token|>'})\n",
    "        assert num_added_toks == 1\n",
    "    \n",
    "    if any([needs_pad_token, needs_sep_token, needs_eos_token, needs_bos_token]):\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "        \n",
    "    return tokenizer, model\n",
    "        \n",
    "    \n"
   ],
   "id": "a45eb832a08e25d9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PhiForSequenceClassification were not initialized from the model checkpoint at microsoft/phi-1_5 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T00:31:47.324844Z",
     "start_time": "2024-06-14T00:31:47.308823Z"
    }
   },
   "cell_type": "code",
   "source": "apply_tokenizer(new_tok, premises, hypotheses)",
   "id": "2868447b1a4b124",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[31373, 1820]], 'attention_mask': [[1, 1]], 'decoded_input_ids': [['hello', 'my']]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T00:36:22.578543Z",
     "start_time": "2024-06-14T00:36:22.129053Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class UpdatedPhiTokenizerFast(CodeGenTokenizerFast):\n",
    "    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n",
    "        \n",
    "        if token_ids_1 is None: \n",
    "            raise NotImplementedError('This method is designed for zero shot classification, so requires a premise and hypothesis to be passed. No token_ids_1 was passed.')\n",
    "        \n",
    "        output = [self.bos_token_id] + token_ids_0 + [self.sep_token_id] + token_ids_1 + [self.eos_token_id]\n",
    "\n",
    "        return output\n",
    "    \n",
    "updated_phi_tok = UpdatedPhiTokenizerFast.from_pretrained(new_repo)\n",
    "\n",
    "updated_phi_tok, phi_model = add_special_tokens_when_missing(updated_phi_tok, phi_model)"
   ],
   "id": "aec91db376bb0eb9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'CodeGenTokenizer'. \n",
      "The class this function is called from is 'UpdatedPhiTokenizerFast'.\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T00:36:24.680023Z",
     "start_time": "2024-06-14T00:36:24.660397Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# this currently throws an error because we have no padding token, so let's add it!\n",
    "apply_tokenizer(updated_phi_tok, premises, hypotheses)"
   ],
   "id": "e0be03b2112f2470",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[31373, 1820]], 'attention_mask': [[1, 1]], 'decoded_input_ids': [['hello', 'my']]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "720b2744c45681e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
